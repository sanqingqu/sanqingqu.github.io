<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sanqing</title>
  
  <meta name="author" content="Sanqing Qu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
   -->
   <link rel="icon" type="image/png" href="images/tongji_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sanqing Qu</name>
              </p>
              
              <p>
                Sanqing Qu (Áûø‰∏âÊ∏Ö) is currently a 3rd-year Ph.D. student in <a href="https://ispc-group.github.io/">Intelligent Sensing, Perception and Computing (ISPC) Gruop</a> lead by <a href="(https://scholar.google.com/citations?user=kBhIyv4AAAAJ&hl=en">Prof. Guang Chen</a> at Tongji University, Shanghai, China.
                 Before that, he received his bachelor degree of Automotive Engineering at Tongji University in 2020. 
              </p>
              
              <p>
                His research interests include autonomous driving, transfer learning, and video analysis.
              </p>

              <p style="text-align:center">
                <a href="mailto:sanqingqu@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/sanqing_pdf.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=pZk-LBIAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/sanqingqu/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:40%">
              <a href="images/sanqing_2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/sanqing_2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <font color="red"> 
              <p> 
                <li style="margin: 10px;">
                  <b>2023.02 :</b>  Our work (GLC) on source-free universal domain adaptation is accepted by CVPR-2023! My favorite work!
                </li> 
                <li style="margin: 10px">
                  <b>2023.02 :</b>  Our work (MAD) on single-domain generation is accepted by CVPR-2023! 
                </li>
              </font> 
                <li style="margin: 10px;">
                  <b>2022.07 :</b> Our work (BMD) on source-free domain adpatation is accepted by ECCV-2022!
                </li>
                <li style="margin: 10px;">
                  <b>2022.03 :</b> One paper is accepted by IEEE T-Cyber (IF=19.118)!
                </li>
                <li style="margin: 10px;">
                  <b>2021.03 :</b> Our work (ACM-Net) on weakly-supervised temporal action localization is released!
                </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>

                <li style="margin: 10px;">
                  <b>2020.09 ~ Present :</b>  PhD student in Automotive Engineering, Tongji University.
                </li>
                <li style="margin: 10px;">
                  <b>2015.09 ~ 2020.07:</b>  Bachelor's Degree in Automotive Engineering, Tongji University.
                </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <p><heading>Selected Publications</heading></p>
            <p>
              * indicates equal contribution
            </p>
          </td>
        </tr>
      </tbody></table>


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/GLC.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Upcycling Models under Domain and Category Shift</papertitle>
            <br>
            <strong>Sanqing Qu*</strong>, Tianpei Zou*, 
            <a href="https://scholar.google.com/citations?user=IEOJBbAAAAAJ&hl=en">Florian R√∂hrbein</a>,
            <a href="https://www.mvig.org/"> Cewu Lu</a>,
            <a href="https://scholar.google.com/citations?user=kBhIyv4AAAAJ&hl=en"> Guang Chen</a>, 
            <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en"> Dacheng Tao </a>,
            Changjun Jiang
            <br>
            <em>IEEE / CVF Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>)</em>, 2023
            <br>
            <!-- <a href="https://arxiv.org/pdf/2204.02811">[arXiv]</a>&nbsp&nbsp
            <a href="https://github.com/ispc-lab/BMD">[Code]</a> -->
            [arXiv] [Code] (coming soon~~~)
            <br>
            <p> Deep neural networks (DNNs) often perform poorly in the presence of domain shift and category shift.
               To address this, in this paper, we explore the Source-free Universal Domain Adaptation (SF-UniDA). 
               SF-UniDA is appealing in view that universal model adaptation can be resolved only on the basis of a standard pre-trained closed-set model,
                i.e., without source raw data and dedicated model architecture.
                Remarkably, in the most challenging open-partial-set DA scenario, GLC outperforms UMAD by 14.8% on the VisDA benchmark.
               </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/MAD.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Modality-Agnostic Debiasing for Single Domain Generalization</papertitle>
            <br>
            <strong>Sanqing Qu</strong>,
            <a href=""> Yingwei Pan</a>, 
            <a href="https://scholar.google.com/citations?user=kBhIyv4AAAAJ&hl=en"> Guang Chen</a>, 
            <a href="https://scholar.google.com/citations?user=7Yc6yssAAAAJ&hl=en"> Ting Yao</a>
            <a href="https://scholar.google.com/citations?user=7Yq4wf4AAAAJ&hl=en"> Tao Mei </a>,
            Changjun Jiang
            <br>
            <em>IEEE / CVF Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>)</em>, 2023
            <br>
            <!-- <a href="https://arxiv.org/pdf/2204.02811">[arXiv]</a>&nbsp&nbsp
            <a href="https://github.com/ispc-lab/BMD">[Code]</a> -->
            [arXiv] [Code] (coming soon~~~)
            <br>
            <p> 
              Existing single-DG techniques commonly devise various data-augmentation algorithms, and remould the multi-source domain generalization methodology
              to learn domain-generalized (semantic) features. Nevertheless, these methods are typically modality-specific,
              thereby being only applicable to one single modality (e.g.,image). In contrast, we target a versatile Modality-Agnostic Debiasing (MAD) framework for single-DG,
              that enables generalization for different modalities. 
              We have evaluated the effectiveness and superiority of MAD for single-DG via various empirical evidences on a series of tasks, including recognition
              on 1D texts, 2D images, 3D point clouds, and semantic segmentation on 2D images. 
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/bmd.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>BMD: A General Class-balanced Multicentric Dynamic Prototype Strategy for Source-free Domain Adaptation</papertitle>
            <br>
            <strong>Sanqing Qu</strong>, 
            <a href="https://scholar.google.com/citations?user=kBhIyv4AAAAJ&hl=en"> Guang Chen</a>, 
            <a href="https://scholar.google.com.hk/citations?user=9jH5v74AAAAJ&hl=en"> Jing Zhang</a>, 
            <a href="https://scholar.google.com/citations?user=xxUvEtIAAAAJ&hl=en"> Zhijun Li </a>, Wei He,
            <a href="https://scholar.google.com/citations?user=RwlJNLcAAAAJ&hl=en"> Dacheng Tao </a>
            <br>
            <em>European Conference on Computer Vision  (<strong>ECCV</strong>)</em>, 2022
            <br>
            <a href="https://arxiv.org/pdf/2204.02811">[arXiv]</a>&nbsp&nbsp
            <a href="https://github.com/ispc-lab/BMD">[Code]</a>
            <br>
            <p> In this paper, we design a general prototype based pseudo-labeling strategy. It is model-agnostic and can be applied to
              existing self-training based SFDA methods.</p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/efall.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>Neuromorphic Vision-based Fall Localization in Event Streams with Temporal‚Äìspatial Attention Weighted Network</papertitle>
            <br>
            <a href="https://scholar.google.com/citations?user=kBhIyv4AAAAJ&hl=en"> Guang Chen</a>*,
            <strong>Sanqing Qu</strong>*,  
            <a href="https://scholar.google.com/citations?user=xxUvEtIAAAAJ&hl=en"> Zhijun Li </a>,
            Haitao Zhu, Jiaxuan Dong, Min Liu,
            <a href="https://scholar.google.com/citations?user=NGdMpTYAAAAJ&hl=en"> Jorg Conradt</a>.
            <br>
            <em>IEEE Transactions on Cybernetics. (<strong>T-Cyber</strong>)</em>, 2022
            <br>
            <a href="https://ieeexplore.ieee.org/abstract/document/9771080/">[IEEE]</a>&nbsp&nbsp
            <!-- <a href="https://github.com/ispc-lab/BMD">[Code]</a> -->
            <br>
            <p> In this paper, we proposed a bio-inspired event-camera based falls temporal localization framework.
              Specifically, we propose a event density-based action proposal generation scheme, and introduce a temporal-spatial attention mechanism for action modeling.
            </p>
            </td>
        </tr>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/acmnet.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <papertitle>ACM-Net: Action Context Modeling Network for Weakly-supervised Temporal Action Localization</papertitle>
            <br>
            <strong>Sanqing Qu</strong>,  
            <a href="https://scholar.google.com/citations?user=kBhIyv4AAAAJ&hl=en"> Guang Chen</a>,
            <a href="https://scholar.google.com/citations?user=xxUvEtIAAAAJ&hl=en"> Zhijun Li </a>,
            Lijun Zhang, 
            <a href="https://scholar.google.com/citations?user=DyEUPFUAAAAJ&hl=en"> Fan Lu </a>,
            <a href="https://scholar.google.com/citations?user=-CA8QgwAAAAJ&hl=en"> Alois Knoll</a>.
            <br>
            Arxiv Pre-print, 2021
            <br>
            <a href="https://arxiv.org/pdf/2104.02967">[arXiv]</a>&nbsp&nbsp
            <a href="https://github.com/ispc-lab/ACM-Net">[Code]</a>
            <br>
            <p> In this paper, we propose an action-context modeling network termed ACM-Net, which integrates a three-branch attention module to measure the likelihood of each temporal point being action instance, context, or non-action background, simultaneously. 
            </p>
            </td>
        </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Honors and Awards</heading>
            <p>
              <li style="margin: 5px;"> <b>2022, 2021 :</b> The Outstanding Doctoral Student Scholarship of Tongji University</li>
              <li style="margin: 5px;"> <b>2020 :</b> The Shanghai Outstanding Graduate</li>
              <li style="margin: 5px;"> <b>2020 :</b> The Second Prize of National Graduate Student Mathematical Modeling Contest</li>
              <li style="margin: 5px;"> <b>2019 :</b> The BaoGang Scholarship (<a href="http://www.bsef.baosteel.com/#/aboutus">ÂÆùÈí¢ÊïôËÇ≤Â•ñ</a>) </li>
              <li style="margin: 5px;"> <b>2018 :</b> Rank 4th in 2018 <a href="https://de.wikipedia.org/wiki/Carolo-Cup">Corolo-Cup</a> of Germany Graduate Students </li>
            </p>
          </td>
        </tr>
      </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <a href="https://jonbarron.info/">Website Template</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>

    <p><center>     
      <br>
        &copy; Sanqing Qu | Last updated: Feb 18, 2023
  </center></p>
  </body>

</body>

</html>
